{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62718269-aa03-47bb-b46f-52b1731871cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: ~/data/CanTho_RoadNetwork.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import heapq\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. C·∫§U H√åNH & D·ªÆ LI·ªÜU\n",
    "# ==============================================================================\n",
    "# ƒê∆∞·ªùng d·∫´n file (B·∫°n s·ª≠a l·∫°i cho ph√π h·ª£p m√°y m√¨nh)\n",
    "INPUT_CSV = '~/data/DataForCmp_GIS.csv'      # File ch·ª©a c√°c b·ªô tham s·ªë Eps, MinPts c·∫ßn test\n",
    "DATA_FILE = '~/data/6_DSCanhKQ2_CanTho_XoaCon3Cot_XoaDongTrung.txt'  # File d·ªØ li·ªáu ƒë·ªì th·ªã (IdStart, IdEnd, Length)\n",
    "OUTPUT_CSV = '~/data/KetQua_Raw_Benchmark.csv'\n",
    "\n",
    "NUM_RUNS = 3  # S·ªë l·∫ßn ch·∫°y l·∫•y trung b√¨nh (ƒë·ªÉ gi·∫£m sai s·ªë ng·∫´u nhi√™n)\n",
    "\n",
    "def load_graph_data(path):\n",
    "    print(f\"--- ƒêANG N·∫†P D·ªÆ LI·ªÜU: {os.path.basename(path)} ---\")\n",
    "    try:\n",
    "        # ƒê·ªçc file (h·ªó tr·ª£ c·∫£ tab v√† space)\n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        if df.shape[1] < 3: df = pd.read_csv(path, delim_whitespace=True)\n",
    "        \n",
    "        df.columns = df.columns.str.strip()\n",
    "        if 'IdStar' in df.columns: df.rename(columns={'IdStar': 'IdStart'}, inplace=True)\n",
    "        \n",
    "        # Chuy·ªÉn sang Dictionary ƒë·ªÉ truy xu·∫•t nhanh O(1)\n",
    "        print(\"üîÑ ƒêang chuy·ªÉn ƒë·ªïi sang Adjacency Dictionary...\")\n",
    "        adj = {}\n",
    "        nodes = set()\n",
    "        for _, row in df.iterrows():\n",
    "            u, v, w = int(row['IdStart']), int(row['IdEnd']), float(row['Length'])\n",
    "            if u not in adj: adj[u] = []\n",
    "            if v not in adj: adj[v] = []\n",
    "            adj[u].append((v, w))\n",
    "            adj[v].append((u, w))\n",
    "            nodes.add(u); nodes.add(v)\n",
    "            \n",
    "        print(f\"‚úÖ ƒê√£ n·∫°p: {len(nodes)} ƒë·ªânh.\")\n",
    "        return adj, list(nodes)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói n·∫°p d·ªØ li·ªáu: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. THU·∫¨T TO√ÅN G·ªêC (NS-DBSCAN STANDARD)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Thu·∫≠t to√°n 1: LSPD G·ªëc (Kh√¥ng c·∫Øt t·ªâa) ---\n",
    "def alg1_LSPD_original(adj, start_node, eps):\n",
    "    distances = {start_node: 0}\n",
    "    queue = [(0, start_node)]\n",
    "    neighbors = []\n",
    "    \n",
    "    while queue:\n",
    "        d, u = heapq.heappop(queue)\n",
    "        \n",
    "        # Ch·ªâ ki·ªÉm tra sau khi ƒë√£ l·∫•y ra kh·ªèi h√†ng ƒë·ª£i\n",
    "        if d > eps: continue \n",
    "        neighbors.append(u)\n",
    "        \n",
    "        if u in adj:\n",
    "            for v, weight in adj[u]:\n",
    "                new_d = d + weight\n",
    "                # V·∫´n c·ªông d·ªìn v√† th√™m v√†o h√†ng ƒë·ª£i d√π c·∫°nh c√≥ th·ªÉ r·∫•t d√†i\n",
    "                if new_d <= eps:\n",
    "                    if new_d < distances.get(v, float('inf')):\n",
    "                        distances[v] = new_d\n",
    "                        heapq.heappush(queue, (new_d, v))\n",
    "    return neighbors\n",
    "\n",
    "# --- Thu·∫≠t to√°n 2: T·∫°o b·∫£ng m·∫≠t ƒë·ªô G·ªëc (L·∫•y t·∫•t c·∫£) ---\n",
    "def alg2_density_ordering_original(adj, points, eps):\n",
    "    neighbors_cache = {}\n",
    "    ordered_list = []\n",
    "    \n",
    "    for p in points:\n",
    "        # Ch·∫°y LSPD cho T·∫§T C·∫¢ c√°c ƒëi·ªÉm\n",
    "        nbrs = alg1_LSPD_original(adj, p, eps)\n",
    "        neighbors_cache[p] = nbrs\n",
    "        # L∆∞u t·∫•t c·∫£ v√†o danh s√°ch, k·ªÉ c·∫£ ƒëi·ªÉm m·∫≠t ƒë·ªô = 0\n",
    "        ordered_list.append((len(nbrs), p))\n",
    "        \n",
    "    # S·∫Øp x·∫øp to√†n b·ªô danh s√°ch (Chi ph√≠ cao n·∫øu danh s√°ch d√†i)\n",
    "    ordered_list.sort(key=lambda x: x[0], reverse=True)\n",
    "    return ordered_list, neighbors_cache\n",
    "\n",
    "# --- Thu·∫≠t to√°n 3: H√¨nh th√†nh c·ª•m (C√≥ ki·ªÉm tra nhi·ªÖu t∆∞·ªùng minh) ---\n",
    "def alg3_clustering_original(ordered_list, neighbors_cache, min_pts):\n",
    "    labels = {} # Dict l∆∞u nh√£n c·ª•m\n",
    "    cluster_id = 0\n",
    "    \n",
    "    # L·∫•y danh s√°ch ƒëi·ªÉm t·ª´ b·∫£ng x·∫øp h·∫°ng\n",
    "    sorted_points = [x[1] for x in ordered_list]\n",
    "    \n",
    "    for p in sorted_points:\n",
    "        if p in labels: continue # ƒê√£ x·ª≠ l√Ω th√¨ b·ªè qua\n",
    "        \n",
    "        p_nbrs = neighbors_cache.get(p, [])\n",
    "        \n",
    "        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán ƒëi·ªÉm l√µi\n",
    "        if len(p_nbrs) >= min_pts:\n",
    "            cluster_id += 1\n",
    "            labels[p] = cluster_id # G√°n nh√£n c·ª•m\n",
    "            \n",
    "            # Lan truy·ªÅn (Expand Cluster)\n",
    "            seeds = list(p_nbrs)\n",
    "            i = 0\n",
    "            while i < len(seeds):\n",
    "                q = seeds[i]\n",
    "                if q not in labels:\n",
    "                    labels[q] = cluster_id\n",
    "                    q_nbrs = neighbors_cache.get(q, [])\n",
    "                    if len(q_nbrs) >= min_pts:\n",
    "                        seeds.extend(q_nbrs)\n",
    "                elif labels[q] == -1: # N·∫øu tr∆∞·ªõc ƒë√≥ b·ªã ƒë√°nh l√† Nhi·ªÖu\n",
    "                    labels[q] = cluster_id # C·ª©u l·∫°i th√†nh bi√™n\n",
    "                i += 1\n",
    "        else:\n",
    "            # G√°n nh√£n NHI·ªÑU t∆∞·ªùng minh (Explicit Noise)\n",
    "            labels[p] = -1 \n",
    "            \n",
    "    return labels\n",
    "\n",
    "# Wrapper ch·∫°y to√†n b·ªô quy tr√¨nh G·ªëc\n",
    "def run_ns_dbscan_original(adj, points, eps, min_pts):\n",
    "    # B∆∞·ªõc 1 & 2\n",
    "    ordered_list, nbr_cache = alg2_density_ordering_original(adj, points, eps)\n",
    "    # B∆∞·ªõc 3\n",
    "    labels = alg3_clustering_original(ordered_list, nbr_cache, min_pts)\n",
    "    return labels\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. THU·∫¨T TO√ÅN C·∫¢I TI·∫æN (iNS-DBSCAN)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Thu·∫≠t to√°n 1: LSPD C·∫£i ti·∫øn (C·∫Øt t·ªâa c·∫°nh - Pruning) ---\n",
    "def alg1_LSPD_improved(adj, start_node, eps):\n",
    "    distances = {start_node: 0}\n",
    "    queue = [(0, start_node)]\n",
    "    neighbors = []\n",
    "    \n",
    "    while queue:\n",
    "        d, u = heapq.heappop(queue)\n",
    "        if d > eps: continue\n",
    "        neighbors.append(u)\n",
    "        \n",
    "        if u in adj:\n",
    "            for v, weight in adj[u]:\n",
    "                # C·∫¢I TI·∫æN 1: C·∫Øt t·ªâa c·∫°nh ngay l·∫≠p t·ª©c\n",
    "                if weight > eps: continue \n",
    "                \n",
    "                new_d = d + weight\n",
    "                if new_d <= eps:\n",
    "                    if new_d < distances.get(v, float('inf')):\n",
    "                        distances[v] = new_d\n",
    "                        heapq.heappush(queue, (new_d, v))\n",
    "    return neighbors\n",
    "\n",
    "# --- Thu·∫≠t to√°n 2: T·∫°o b·∫£ng m·∫≠t ƒë·ªô C·∫£i ti·∫øn (L·ªçc ng∆∞·ª°ng Logarit) ---\n",
    "def alg2_density_ordering_improved(adj, points, eps):\n",
    "    neighbors_cache = {}\n",
    "    ordered_list = []\n",
    "    \n",
    "    # C·∫¢I TI·∫æN 2A: T√≠nh ng∆∞·ª°ng l·ªçc Heuristic\n",
    "    n = len(points)\n",
    "    threshold = math.log(n) if n > 0 else 0\n",
    "    \n",
    "    for p in points:\n",
    "        nbrs = alg1_LSPD_improved(adj, p, eps)\n",
    "        \n",
    "        # C·∫¢I TI·∫æN 2B: Ch·ªâ th√™m v√†o b·∫£ng n·∫øu m·∫≠t ƒë·ªô >= ng∆∞·ª°ng\n",
    "        if len(nbrs) >= threshold:\n",
    "            neighbors_cache[p] = nbrs\n",
    "            ordered_list.append((len(nbrs), p))\n",
    "        # C√°c ƒëi·ªÉm < threshold b·ªã LO·∫†I B·ªé ho√†n to√†n, ti·∫øt ki·ªám b·ªô nh·ªõ & th·ªùi gian sort\n",
    "            \n",
    "    # S·∫Øp x·∫øp danh s√°ch (Danh s√°ch n√†y ng·∫Øn h∆°n b·∫£n g·ªëc nhi·ªÅu)\n",
    "    ordered_list.sort(key=lambda x: x[0], reverse=True)\n",
    "    return ordered_list, neighbors_cache\n",
    "\n",
    "# --- Thu·∫≠t to√°n 3: H√¨nh th√†nh c·ª•m (Nhi·ªÖu ng·∫ßm ƒë·ªãnh) ---\n",
    "def alg3_clustering_improved(ordered_list, neighbors_cache, min_pts):\n",
    "    labels = {}\n",
    "    cluster_id = 0\n",
    "    \n",
    "    sorted_points = [x[1] for x in ordered_list]\n",
    "    \n",
    "    for p in sorted_points:\n",
    "        if p in labels: continue\n",
    "        \n",
    "        p_nbrs = neighbors_cache.get(p, [])\n",
    "        \n",
    "        if len(p_nbrs) >= min_pts:\n",
    "            cluster_id += 1\n",
    "            labels[p] = cluster_id\n",
    "            \n",
    "            seeds = list(p_nbrs)\n",
    "            i = 0\n",
    "            while i < len(seeds):\n",
    "                q = seeds[i]\n",
    "                # C·∫¢I TI·∫æN 3: Ch·ªâ quan t√¢m g√°n nh√£n c·ª•m\n",
    "                # Kh√¥ng t·ªën th·ªùi gian ki·ªÉm tra/g√°n l·∫°i nh√£n Noise (-1)\n",
    "                if q not in labels:\n",
    "                    labels[q] = cluster_id\n",
    "                    q_nbrs = neighbors_cache.get(q, [])\n",
    "                    if len(q_nbrs) >= min_pts:\n",
    "                        seeds.extend(q_nbrs)\n",
    "                i += 1\n",
    "        # Kh√¥ng c√≥ d√≤ng \"else: labels[p] = -1\" -> Nhi·ªÖu ng·∫ßm ƒë·ªãnh\n",
    "        \n",
    "    return labels\n",
    "\n",
    "# Wrapper ch·∫°y to√†n b·ªô quy tr√¨nh C·∫£i ti·∫øn\n",
    "def run_ns_dbscan_improved(adj, points, eps, min_pts):\n",
    "    ordered_list, nbr_cache = alg2_density_ordering_improved(adj, points, eps)\n",
    "    labels = alg3_clustering_improved(ordered_list, nbr_cache, min_pts)\n",
    "    return labels\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. BENCHMARK ENGINE (RAW - KH√îNG CHE)\n",
    "# ==============================================================================\n",
    "def measure_time(func, *args):\n",
    "    gc.collect() # D·ªçn r√°c b·ªô nh·ªõ ƒë·ªÉ ƒëo ch√≠nh x√°c h∆°n\n",
    "    start = time.time()\n",
    "    func(*args)\n",
    "    end = time.time()\n",
    "    return (end - start) * 1000 # ƒê·ªïi ra ms\n",
    "\n",
    "def run_raw_benchmark():\n",
    "    # 1. N·∫°p d·ªØ li·ªáu\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {DATA_FILE}\")\n",
    "        return\n",
    "    \n",
    "    adj, nodes = load_graph_data(DATA_FILE)\n",
    "    if not adj: return\n",
    "\n",
    "    # 2. T·∫°o file tham s·ªë m·∫´u n·∫øu ch∆∞a c√≥\n",
    "    if not os.path.exists(INPUT_CSV):\n",
    "        pd.DataFrame({\n",
    "            'Eps': [200, 300, 400, 500],\n",
    "            'MinPts': [10, 15, 20, 25]\n",
    "        }).to_csv(INPUT_CSV, index=False)\n",
    "        print(\"‚ö†Ô∏è ƒê√£ t·∫°o file tham s·ªë m·∫´u.\")\n",
    "\n",
    "    df_params = pd.read_csv(INPUT_CSV)\n",
    "    results = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(f\"{'BENCHMARK TRUNG TH·ª∞C (RAW RESULTS)':^85}\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"| {'Eps':<6} | {'MinPts':<6} | {'G·ªëc (ms)':<15} | {'C·∫£i ti·∫øn (ms)':<15} | {'Nhanh h∆°n (%)':<15} |\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for _, row in df_params.iterrows():\n",
    "        eps = float(row['Eps'])\n",
    "        min_pts = int(row['MinPts'])\n",
    "        \n",
    "        total_orig = 0\n",
    "        total_imp = 0\n",
    "        \n",
    "        # Ch·∫°y nhi·ªÅu l·∫ßn l·∫•y trung b√¨nh\n",
    "        for _ in range(NUM_RUNS):\n",
    "            total_orig += measure_time(run_ns_dbscan_original, adj, nodes, eps, min_pts)\n",
    "            total_imp += measure_time(run_ns_dbscan_improved, adj, nodes, eps, min_pts)\n",
    "            \n",
    "        avg_orig = total_orig / NUM_RUNS\n",
    "        avg_imp = total_imp / NUM_RUNS\n",
    "        \n",
    "        # T√≠nh % c·∫£i thi·ªán (D∆∞∆°ng l√† nhanh h∆°n, √Çm l√† ch·∫≠m h∆°n)\n",
    "        improvement = ((avg_orig - avg_imp) / avg_orig) * 100\n",
    "        \n",
    "        print(f\"| {int(eps):<6} | {min_pts:<6} | {avg_orig:<15.2f} | {avg_imp:<15.2f} | {improvement:<15.2f} |\")\n",
    "        \n",
    "        results.append({\n",
    "            'Eps': eps,\n",
    "            'MinPts': min_pts,\n",
    "            'Original_Time': avg_orig,\n",
    "            'Improved_Time': avg_imp,\n",
    "            'Improvement_Percent': improvement\n",
    "        })\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(\"=\"*85)\n",
    "    print(f\"‚úÖ K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i: {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_raw_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86312a49-98c4-4674-9cec-c7cbadb673b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
